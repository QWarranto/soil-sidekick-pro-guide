# Baseline Performance Metrics
# LeafEngines‚Ñ¢ B2B API Platform

## Version: 2.2
## Date: February 2026

Record baseline metrics before optimization to measure improvements.

## Test Date: 2026-02-05
## Environment: Production (Supabase Edge Functions)
## Tester: Automated Validation

---

## üü¢ SUB-100MS SLA ‚Äî OFFLINE LOCAL INFERENCE

**Test Date**: February 5, 2026

### Scope Clarification

The **sub-100ms latency SLA** applies to **offline local inference** using WebGPU and Google Gemma models ‚Äî NOT online cloud API round-trips. This positioning is strategic for:

- **Remote agricultural locations** with unreliable or no internet connectivity
- **Privacy-first European markets** requiring local data processing (GDPR compliance)
- **Cost-sensitive deployments** avoiding per-query cloud API costs

### Offline Inference Targets (WebGPU/Gemma)

| Use Case | Target | Technology | Status |
|----------|--------|------------|--------|
| Plant identification | <100ms | Gemma 2B + WebGPU | ‚úÖ Target |
| Report summarization | <200ms | Gemma 2B + WebGPU | ‚úÖ Target |
| Plant health diagnosis | <150ms | Gemma 2B + WebGPU | ‚úÖ Target |
| Care advice generation | <100ms | Gemma 2B + WebGPU | ‚úÖ Target |
| Model cold start (first load) | <5s | ONNX model download | Expected |

### Architecture: Hybrid Online/Offline

LeafEngines uses a **hybrid approach**:

1. **Offline-first (WebGPU)**: Sub-100ms local inference for core AI features
2. **Online sync**: Background data synchronization when connectivity available
3. **Graceful degradation**: Full functionality without internet access
4. **CPU fallback**: Automatic fallback to CPU if WebGPU unavailable

### Online API Performance (Cloud) ‚Äî Separate SLA

Online API endpoints (requiring database access) operate under different expectations:

| Endpoint Category | Target | Notes |
|-------------------|--------|-------|
| Cached county lookups | <1000ms | Database-backed caching |
| Soil data queries | <1500ms | External API integration |
| Agricultural intelligence | <3000ms | GPT-4 API calls |

> **Note**: Cloud API latency inherently includes network round-trip, database queries, and edge function processing. Sub-100ms is not the target for online operations.

---

## 1. County Lookup

**Test Command**: `k6 run scripts/test-county-lookup.js`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs (concurrent users) | 100 | | 100 | |
| Test duration | 9 min | | 9 min | |
| Total requests | | | >10,000 | |
| Requests/sec | | | >20 | |
| Avg response time | | | <500ms | |
| p95 response time | | | <1000ms | ‚úÖ/‚ùå |
| p99 response time | | | <2000ms | ‚úÖ/‚ùå |
| Error rate | | | <1% | ‚úÖ/‚ùå |
| Cost per 1000 requests | | | <$0.50 | ‚úÖ/‚ùå |

**Notes**:
- Cache hit rate: ____%
- Database connections used: ___
- Rate limit violations: ___

---

## 2. Soil Data Retrieval

**Test Command**: `k6 run scripts/test-soil-data.js`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs | 80 | | 80 | |
| Total requests | | | >5,000 | |
| Avg response time | | | <1500ms | |
| p95 response time | | | <3000ms | ‚úÖ/‚ùå |
| Error rate | | | <2% | ‚úÖ/‚ùå |
| Cost per 1000 requests | | | <$1.00 | ‚úÖ/‚ùå |

**Query Performance**:
- Slowest query: _______________
- Query time: ___ms
- Missing indexes identified: ___

---

## 3. Cost Monitoring

**Test Command**: `k6 run scripts/test-cost-monitoring.js`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs | 150 | | 150 | |
| Total writes | | | >10,000 | |
| Avg response time | | | <200ms | |
| p95 response time | | | <500ms | ‚úÖ/‚ùå |
| Error rate | | | <1% | ‚úÖ/‚ùå |
| Writes per second | | | >50 | ‚úÖ/‚ùå |

**Database Impact**:
- Connection pool saturation: ____%
- Lock contention detected: Yes/No
- Index write overhead: ___ms

---

## 4. Visual Crop Analysis (‚ö†Ô∏è Expensive)

**Test Command**: `k6 run scripts/test-visual-crop-analysis.js --vus 5 --duration 2m`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs | 5 | | 5 | |
| Total requests | | | >20 | |
| Avg response time | | | <8000ms | |
| p95 response time | | | <15000ms | ‚úÖ/‚ùå |
| Error rate | | | <10% | ‚úÖ/‚ùå |
| Cost per request | | | <$0.50 | ‚úÖ/‚ùå |
| Rate limit hits (429) | | | Expected | |

**Cost Analysis**:
- Total cost for test: $___
- Projected monthly cost (10k users): $___

---

## 5. Agricultural Intelligence (‚ö†Ô∏è Most Expensive)

**Test Command**: `k6 run scripts/test-agricultural-intelligence.js --vus 2 --duration 1m`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs | 2 | | 2 | |
| Total requests | | | >10 | |
| Avg response time | | | <10000ms | |
| p95 response time | | | <20000ms | ‚úÖ/‚ùå |
| Error rate | | | <10% | ‚úÖ/‚ùå |
| Cost per request | | | <$0.20 | ‚úÖ/‚ùå |
| GPT-4 tokens avg | | | <3000 | |

**AI Performance**:
- Average prompt tokens: ___
- Average completion tokens: ___
- Total cost for test: $___
- Projected monthly cost (1k AI queries): $___

---

## 6. LeafEngines Query (B2B API)

**Test Command**: `k6 run scripts/test-leafengines-query.js --vus 50 --duration 5m`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| VUs | 50 | | 50 | |
| Total requests | | | >2,000 | |
| Avg response time | | | <2000ms | |
| p95 response time | | | <4000ms | ‚úÖ/‚ùå |
| Error rate | | | <2% | ‚úÖ/‚ùå |
| Rate limit violations | | | <5% | ‚úÖ/‚ùå |

**B2B API Performance**:
- x-api-key validation time: ___ms
- Average query processing: ___ms
- Cache hit rate: ___%

---

## 7. Service Resilience (Retry Logic)

**Test Command**: `k6 run scripts/test-retry-logic.js --vus 20 --duration 3m`

| Metric | Baseline | Current | Target | Status |
|--------|----------|---------|--------|--------|
| Total requests | | | >500 | |
| Retry attempts triggered | | | >50 | |
| Retry success rate | | | >70% | ‚úÖ/‚ùå |
| Average retry delay | | | 1-4s | ‚úÖ/‚ùå |
| User-facing errors | | | <5% | ‚úÖ/‚ùå |

**Resilience Metrics**:
- Circuit breaker activations: ___
- Fallback executions: ___
- Graceful degradation success: ___%

---

## Infrastructure Metrics

### Database
- Max connections: ___/100
- Average connection time: ___ms
- Slow query count (>1s): ___
- Deadlocks: ___
- Failed transactions: ___

### Edge Functions
- Cold start frequency: ___%
- Average cold start time: ___ms
- Memory usage peak: ___MB
- CPU throttling events: ___

### Caching
- Cache hit rate: ___%
- Cache miss penalty (avg): ___ms
- Cache eviction rate: ___/min

### Rate Limiting (December 2025)
- Free tier violations: ___/day
- Starter tier violations: ___/day
- Pro tier violations: ___/day
- Enterprise tier violations: ___/day

---

## Bottlenecks Identified

### Critical Issues (Must Fix)
1. 
2. 
3. 

### Performance Improvements Needed
1. 
2. 
3. 

### Cost Optimization Opportunities
1. 
2. 
3. 

---

## Optimization Action Items

| Priority | Action | Expected Improvement | Assigned To | Status |
|----------|--------|---------------------|-------------|--------|
| P0 | | | | |
| P1 | | | | |
| P2 | | | | |

---

## Next Test Date: [YYYY-MM-DD]

**Retest After**:
- [ ] Database indexes added
- [ ] Query optimization completed
- [ ] Caching layer implemented
- [ ] Prompt engineering for AI cost reduction
- [ ] Rate limiting tuned
- [ ] Connection pooling optimized

---

## Comparison with Previous Test

| Metric | Previous | Current | Change | Trend |
|--------|----------|---------|--------|-------|
| Overall availability | | | | |
| Avg response time | | | | |
| Total cost per 1k users | | | | |
| Error rate | | | | |

**Summary**: 
- Improvements: 
- Regressions: 
- Action needed: 

---

**Tester Signature**: _______________  
**Date**: _______________
